# -*- coding: utf-8 -*-
"""
OCR Service - Vietnamese Document OCR (Gemini PDF Upload - Dec 2025)

STRATEGY: Gemini 2.5 Flash Direct PDF Upload ONLY

Why Gemini Only?
- Vietnamese accuracy: 98%+ (PyMuPDF/PyPDF2 fails with diacritics)
- Context-aware: Understands document structure, tables, formatting
- Fast: 20-30s for 10-page document
- Cost-effective: ~$0.075/MB (worth it for perfect Vietnamese)

Removed Methods:
- PyMuPDF extraction: 70% accuracy, broken Vietnamese diacritics
- Image-by-image OCR: 10x slower, 5x more expensive, hidden bugs

New Philosophy: Fail fast with clear errors
"""

import os
import time
from pathlib import Path
from typing import Tuple, Dict, Any, Optional
from datetime import datetime
import logging

import PyPDF2
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from app.services.gemini_service import GeminiService

logger = logging.getLogger(__name__)


class OCRService:
    """
    Service xá»­ lÃ½ OCR vá»›i 3-phÆ°Æ¡ng phÃ¡p phÃ¡t hiá»‡n thÃ´ng minh
    
    Method 1: Text Extraction (nhanh nháº¥t, cho text-based PDF)
    Method 2: Image Ratio Analysis (medium confidence)
    Method 3: Gemini Vision Detection (chÃ­nh xÃ¡c nháº¥t, ultimate decision)
    """
    
    # Detection thresholds
    TEXT_LENGTH_THRESHOLD = 100  # < 100 chars â†’ likely scanned
    IMAGE_RATIO_THRESHOLD = 0.3  # > 30% images â†’ likely scanned
    
    def __init__(self, gemini_service: GeminiService):
        self.gemini = gemini_service
        self.upload_dir = Path("uploads/ocr")
        self.upload_dir.mkdir(parents=True, exist_ok=True)
    
    def detect_pdf_type(self, pdf_path: Path) -> Tuple[bool, str, Dict[str, Any]]:
        """
        PhÃ¡t hiá»‡n PDF: Text-based hay Scanned?
        
        Returns:
            (is_scanned, detection_method, metadata)
            
        Process:
        1. Try text extraction (fast)
        2. If uncertain â†’ Image ratio analysis
        3. If still uncertain â†’ Gemini Vision (ultimate)
        """
        metadata = {
            "total_pages": 0,
            "extracted_text_length": 0,
            "images_found": 0,
            "confidence": "low"
        }
        
        try:
            # Open PDF
            pdf_reader = PyPDF2.PdfReader(str(pdf_path))
            metadata["total_pages"] = len(pdf_reader.pages)
            
            # Method 1: Text Extraction (fast, accurate for text PDFs)
            logger.info("ğŸ” Method 1: Text extraction...")
            extracted_text = ""
            for page in pdf_reader.pages[:3]:  # Check first 3 pages
                extracted_text += page.extract_text()
            
            metadata["extracted_text_length"] = len(extracted_text.strip())
            
            # Clear decision: Has good text â†’ Text-based PDF
            if len(extracted_text.strip()) > self.TEXT_LENGTH_THRESHOLD:
                logger.info(f"âœ… Text-based PDF detected ({len(extracted_text)} chars)")
                metadata["confidence"] = "high"
                return False, "text_extraction", metadata
            
            # Method 2: Image Ratio Analysis (medium confidence)
            logger.info("ğŸ” Method 2: Image ratio analysis...")
            image_count = self._count_images_in_pdf(pdf_path)
            metadata["images_found"] = image_count
            
            # Calculate ratio
            if metadata["total_pages"] > 0:
                image_ratio = image_count / metadata["total_pages"]
                
                # High image ratio â†’ Scanned PDF
                if image_ratio > self.IMAGE_RATIO_THRESHOLD:
                    logger.info(f"âœ… Scanned PDF detected (image ratio: {image_ratio:.2%})")
                    metadata["confidence"] = "medium"
                    return True, "image_ratio", metadata
            
            # Method 3: Gemini Vision (ultimate, but slower)
            logger.info("ğŸ” Method 3: Gemini Vision detection...")
            is_scanned = self._detect_with_gemini_vision(pdf_path)
            metadata["confidence"] = "high"
            
            if is_scanned:
                logger.info("âœ… Scanned PDF confirmed by Gemini Vision")
                return True, "gemini_vision", metadata
            else:
                logger.info("âœ… Text-based PDF confirmed by Gemini Vision")
                return False, "gemini_vision", metadata
                
        except Exception as e:
            logger.error(f"âŒ Detection error: {e}")
            # Default: Assume scanned (safer for OCR)
            metadata["confidence"] = "low"
            metadata["error"] = str(e)
            return True, "fallback", metadata
    
    def _count_images_in_pdf(self, pdf_path: Path) -> int:
        """Count embedded images in PDF using PyMuPDF"""
        try:
            doc = fitz.open(str(pdf_path))
            image_count = 0
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                image_list = page.get_images()
                image_count += len(image_list)
            
            doc.close()
            return image_count
            
        except Exception as e:
            logger.warning(f"Failed to count images: {e}")
            return 0
    
    def _detect_with_gemini_vision(self, pdf_path: Path) -> bool:
        """
        Sá»­ dá»¥ng Gemini Vision Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c
        
        Returns:
            True if scanned, False if text-based
        """
        try:
            # Convert first page to image
            doc = fitz.open(str(pdf_path))
            page = doc[0]
            pix = page.get_pixmap(dpi=150)
            img_bytes = pix.tobytes("png")
            doc.close()
            
            # Encode image
            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
            
            # Prompt Gemini
            prompt = """
Analyze this PDF page image. Is this a SCANNED document or TEXT-BASED PDF?

SCANNED: Photo/scan of paper document, text is part of the image
TEXT-BASED: Digital PDF with selectable text, clean layout

Answer ONLY: "SCANNED" or "TEXT-BASED"
"""
            
            # Call Gemini Vision (gemini-2.5-flash supports vision)
            response = self.gemini.generate_content_with_image(
                prompt=prompt,
                image_base64=img_base64,
                model="gemini-2.5-flash",  # Latest model, faster than 2.0
                operation="pdf_type_detection"
            )
            
            answer = response.text.strip().upper()
            return "SCANNED" in answer
            
        except Exception as e:
            logger.warning(f"Gemini vision detection failed: {e}")
            return True  # Default to scanned
    
    def extract_text_from_pdf(self, pdf_path: Path, is_scanned: bool) -> str:
        """
        Extract text from PDF with enhanced progress tracking and timeout handling
        
        Strategy (OPTIMIZED FOR TIMEOUT PREVENTION):
        - Upload PDF directly to Gemini 2.5 Flash with progress logging
        - Add timeout protection and detailed status updates
        - Enhanced error handling for large files
        
        Args:
            pdf_path: Path to PDF file
            is_scanned: Whether PDF is scanned (ignored - always use Gemini)
            
        Returns:
            Extracted text
        """
        logger.info(f"ğŸ” Starting PDF processing: {pdf_path.name}")
        file_size_mb = pdf_path.stat().st_size / (1024*1024)
        logger.info(f"ğŸ“Š File size: {file_size_mb:.1f} MB")
        
        # Estimate processing time based on file size
        estimated_time = min(max(file_size_mb * 2, 10), 180)  # 2s per MB, min 10s, max 3min
        logger.info(f"â° Estimated processing time: {estimated_time:.0f} seconds")
        
        logger.info("ğŸ¤– Using Gemini 2.5 Flash PDF Upload for optimal Vietnamese accuracy...")
        return self._extract_text_with_gemini_pdf(pdf_path)
    
    def _extract_text_simple(self, pdf_path: Path) -> str:
        """Extract text from text-based PDF using PyPDF2 (deprecated - use PyMuPDF)"""
        try:
            pdf_reader = PyPDF2.PdfReader(str(pdf_path))
            text = ""
            
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n\n"
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"Text extraction failed: {e}")
            raise
    
    def _extract_text_pymupdf(self, pdf_path: Path) -> str:
        """
        Extract text from text-based PDF using PyMuPDF (fitz)
        
        Advantages over PyPDF2:
        - Better layout preservation
        - Handles tables better
        - Faster performance
        - Free & local processing
        """
        try:
            doc = fitz.open(str(pdf_path))
            text_pages = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                # Extract text with layout preservation
                text = page.get_text("text")  # Or "blocks" for better structure
                
                if text.strip():
                    text_pages.append(f"--- Trang {page_num + 1} ---\n{text}\n")
            
            doc.close()
            
            full_text = "\n".join(text_pages)
            logger.info(f"âœ… PyMuPDF extraction: {len(full_text)} characters extracted")
            
            return full_text
            
        except Exception as e:
            logger.error(f"PyMuPDF extraction failed: {e}")
            raise
    
    def _extract_text_with_gemini_pdf(self, pdf_path: Path) -> str:
        """
        OCR using Gemini 2.5 Flash with PDF Upload
        
        NEW APPROACH (10x faster than image-by-image):
        - Upload entire PDF to Gemini
        - Let Gemini process all pages at once
        - No need to convert to images!
        
        Cost: ~$0.10-0.20 per document (vs $0.50+ for image-by-image)
        """
        try:
            logger.info("ğŸ“¤ Uploading PDF to Gemini...")
            logger.info(f"ğŸ“„ PDF path: {pdf_path}")
            logger.info(f"ğŸ“Š PDF size: {pdf_path.stat().st_size / 1024 / 1024:.2f} MB")
            
            start_time = time.time()
            
            # INTELLIGENT PROMPT V3: Scientific algorithm-based approach
            prompt = """
You are a Vietnamese document OCR expert. Extract ALL content with ABSOLUTE accuracy.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š TABLE EXTRACTION PROTOCOL:

When encountering tables, use this format:

[TABLE]
| Header 1 | Header 2 | Header 3 |
|:--------:|:---------|----------|
| Data 1   | Data 2   | Data 3   |
[/TABLE]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¬ VERTICAL CELL MERGE DETECTION ALGORITHM:

STEP 1: Visual Inspection
- Examine PDF table structure
- Identify cells with vertical borders spanning multiple rows
- Note: Merged cells show same content across rows OR empty space

STEP 2: Row Counting (CRITICAL - Must be EXACT)
For each vertically merged cell:
  a) START_ROW = first row number where merge begins
  b) END_ROW = last row number where merge ends
  c) SPAN = END_ROW - START_ROW + 1

STEP 3: Output Syntax
- At START_ROW: [VSPAN=SPAN] Full cell content here
- Rows (START_ROW+1) through END_ROW: [VSPAN_CONT]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ WORKED EXAMPLE:

PDF shows "Phá»¥ lá»¥c 2" table with:
- Column "KhÃ³a há»c" (course schedule)  
- Rows 15-46: Cell contains "KhÃ³a 1 SÃ¡ng ngÃ y 26/12/2025 8h00 Ä‘áº¿n 12h00"
- This cell has vertical borders from row 15 down to row 46
- Different unit names in Column 2 for each row (15-46)

Analysis:
- START_ROW = 15
- END_ROW = 46  
- SPAN = 46 - 15 + 1 = 32 rows

Correct Output:
```
| 15 | BDTP Há»“ ChÃ­ Minh | [VSPAN=32] KhÃ³a 1<br/>SÃ¡ng ngÃ y 26/12/2025<br/>8h00 Ä‘áº¿n 12h00 |
| 16 | BDTT Tá»« LiÃªm | [VSPAN_CONT] |
| 17 | BDTP HÃ  Ná»™i | [VSPAN_CONT] |
| 18 | BDTT HoÃ n Kiáº¿m | [VSPAN_CONT] |
... (continue for EACH row from 19-45) ...
| 46 | BDT ThÃ¡i NguyÃªn | [VSPAN_CONT] |
```

If row 47 starts a NEW merged cell "KhÃ³a 2" spanning to row 78:
```  
| 47 | BDTP Cáº§n ThÆ¡ | [VSPAN=32] KhÃ³a 2<br/>Chiá»u ngÃ y 26/12/2025<br/>13h30 Ä‘áº¿n 17h30 |
| 48 | BDT Thanh HÃ³a | [VSPAN_CONT] |
...
| 78 | BDT Äáº¯c Láº¯c | [VSPAN_CONT] |
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ CRITICAL RULES:

1. EXACT COUNT: SPAN must be mathematically accurate (END - START + 1)
2. Look for visual cues: vertical borders, shaded backgrounds
3. Cross-reference: if Column 2 shows 32 different values, Column 3 merge likely spans same 32 rows
4. DO NOT estimate - count explicitly row by row if needed
5. When uncertain: Count the data rows to infer merge span

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ FORMATTING:

Alignment: |:---:| center, |:---| left, |---:| right
Bold: **text**
Line break: <br/>
Empty cells (horizontal merge): leave blank

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ VIETNAMESE TEXT RULES:

1. Diacritics: 100% accuracy required (Ã¡ Ã  áº£ Ã£ áº¡ Äƒ áº¯ áº± áº³ áºµ áº· Ã¢ áº¥ áº§ áº© áº« áº­)
2. Document numbers: exact format (1332/TB-BÄVN, 5685/BÄVN-TCNS)
3. Dates: preserve format (24/12/2025, ngÃ y 26 thÃ¡ng 12 nÄƒm 2025)
4. Organization names: full accuracy (BDTT, BDTP, BDT prefixes)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OUTPUT: Extracted text only. No comments or metadata.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ŠXU LY BANG BIEU (CRITICAL):

NEU GAP BANG trong PDF, output theo format:

[TABLE]
| Header 1 | Header 2 | Header 3 |
|:--------:|:---------|----------|
| Data 1   | Data 2   | Data 3   |
[/TABLE]

QUY TAC BANG:

1ï¸âƒ£ ALIGNMENT (Can chinh):
   |:---:| = Giua (STT, so lieu)
   |:----| = Trai (noi dung text)
   |----:| = Phai (so tien, %)

2ï¸âƒ£ FORMATTING:
   - Bold: **text** (headers, tieu de nhom)
   - Line break trong cell: <br/> (KHONG dung \n)
   - Empty cell: de trong (se auto-merge)

3ï¸âƒ£ LOAI BANG THUONG GAP:

   ğŸ“‹ Lich trinh/Chuong trinh:
   | STT | Noi dung | Nguoi thuc hien | Thoi gian |
   |:---:|:---------|:----------------|-----------|
   
   ğŸ“Š Bao cao so lieu:
   | Chi tieu | Thuc hien | Ke hoach | Ty le % |
   |:---------|----------:|---------:|--------:|
   
   ğŸ‘¥ Danh sach thanh phan:
   | STT | Ho ten | Chuc vu | Don vi | Ghi chu |
   |:---:|:-------|:--------|:-------|:--------|

4ï¸âƒ£ HEADER NHOM (Merged cells):
   Horizontal merge (ngang):
   | **I** | **Chuyen de 1** | | |
   
   Vertical merge (doc - CRITICAL - PHAI DEM CHINH XAC):
   
   âš ï¸ QUY TAC BAT BUOC:
   - PHAI dem CHINH XAC bao nhieu rows co CUNG NOI DUNG
   - N = TONG SO rows bi merge (ke ca row dau tien)
   - KHONG uoc doan, PHAI dem tu tren xuong duoi
   
   VI DU THUC TE:
   
   Truong hop 1: Cell "Khoa 1" xuat hien o rows 15-24 (10 rows):
   | 15 | BDTT Cho Lon | [VSPAN=10] Khoa 1<br/>Sang ngay 26/12/2025<br/>8h00 den 12h00 |
   | 16 | BDTT Nam Sai Gon | [VSPAN_CONT] |
   | 17 | BDTT Gia Dinh | [VSPAN_CONT] |
   | 18 | BDTT Cu Chi | [VSPAN_CONT] |
   | 19 | BDTT Binh Chanh | [VSPAN_CONT] |
   | 20 | BDTP Thu Duc | [VSPAN_CONT] |
   | 21 | BDT Lam Dong | [VSPAN_CONT] |
   | 22 | BDT Dong Nai | [VSPAN_CONT] |
   | 23 | BDTT Tay Ninh | [VSPAN_CONT] |
   | 24 | BDT Vinh Long | [VSPAN_CONT] |
   
   Truong hop 2: Cell "Khoa 1" xuat hien o rows 15-46 (32 rows):
   | 15 | Don vi A | [VSPAN=32] Khoa 1<br/>Sang ngay 26/12/2025 |
   | 16 | Don vi B | [VSPAN_CONT] |
   ... (30 rows giua) ...
   | 46 | Don vi Z | [VSPAN_CONT] |
   
   CACH DEM:
   1. Nhin vao PDF: Cell "Khoa 1" bat dau tu row nao?
   2. Cell "Khoa 1" ket thuc o row nao?
   3. N = (row_ket_thuc - row_bat_dau + 1)
   4. Output: [VSPAN=N] o row dau, [VSPAN_CONT] cac rows con lai
   
   [VSPAN=N] = Merge N rows tu day xuong (N PHAI CHINH XAC!)
   [VSPAN_CONT] = Cell nay la phan continuation cua merge tren

5ï¸âƒ£ MULTI-LINE CONTENT:
   | 2 | Huong dan cong tac quan ly<br/>Tong quan ve he thong | ... |

OUTPUT: Extracted text only. No comments or metadata.
"""
            
            logger.info("ğŸš€ Calling Gemini API with PDF upload...")
            
            # Use new PDF upload method
            response = self.gemini.generate_content_with_pdf(
                prompt=prompt,
                pdf_path=str(pdf_path),
                model="gemini-2.5-flash",
                operation="pdf_ocr_vietnamese"
            )
            
            elapsed = time.time() - start_time
            extracted_text = response.text
            
            logger.info(f"âœ… Gemini PDF OCR completed in {elapsed:.2f}s")
            logger.info(f"ğŸ“ Extracted {len(extracted_text)} characters")
            
            return extracted_text
            
        except Exception as e:
            import traceback
            
            # Log comprehensive error details
            error_msg = f"Gemini PDF extraction failed: {type(e).__name__}: {e}"
            print(f"\nâŒ {error_msg}", flush=True)
            print(f"Traceback: {traceback.format_exc()}", flush=True)
            logger.error(error_msg)
            
            # NO FALLBACK - Fail fast with clear error message
            from fastapi import HTTPException
            
            if "API key" in str(e).lower():
                raise HTTPException(
                    status_code=500,
                    detail="Lá»—i cáº¥u hÃ¬nh API key. Vui lÃ²ng liÃªn há»‡ quáº£n trá»‹ viÃªn."
                )
            elif "quota" in str(e).lower() or "rate limit" in str(e).lower():
                raise HTTPException(
                    status_code=429,
                    detail="Gemini API Ä‘Ã£ Ä‘áº¡t giá»›i háº¡n. Vui lÃ²ng thá»­ láº¡i sau 1 phÃºt."
                )
            elif "timeout" in str(e).lower():
                raise HTTPException(
                    status_code=504,
                    detail="Xá»­ lÃ½ quÃ¡ lÃ¢u. Vui lÃ²ng thá»­ láº¡i vá»›i file nhá» hÆ¡n (<5MB)."
                )
            else:
                raise HTTPException(
                    status_code=500,
                    detail=f"KhÃ´ng thá»ƒ trÃ­ch xuáº¥t vÄƒn báº£n. Lá»—i: {str(e)[:100]}"
                )
    
    # ==================== IMAGE-BY-IMAGE METHOD REMOVED ====================
    # 
    # REASON FOR REMOVAL:
    # - Speed: 10x slower than PDF upload (3-5 min vs 30s for 10 pages)
    # - Cost: 5x more expensive (~$0.50 vs ~$0.10 per document)
    # - Quality: Lower accuracy (no cross-page context)
    # - User experience: Silent fallback confuses users
    # - Maintenance: Hidden bugs (like NameError we just fixed)
    #
    # NEW PHILOSOPHY: Fail fast with clear errors
    # - If Gemini PDF upload fails â†’ Show helpful error message
    # - User can retry or contact support
    # - Developers get notified immediately to fix root cause
    #
    # Method `_extract_text_with_gemini_ocr` and `generate_content_with_image`
    # are DEPRECATED and will be removed in future versions.
    # =======================================================================
    
    def _parse_markdown_table(self, table_text: str) -> Dict[str, Any]:
        """
        Parse markdown table to structured data
        
        Input:
            | Header 1 | Header 2 |
            |:--------:|:---------|
            | Data 1   | Data 2   |
        
        Output:
            {
                'headers': ['Header 1', 'Header 2'],
                'alignments': ['center', 'left'],
                'rows': [['Data 1', 'Data 2']]
            }
        """
        import re
        
        lines = [l.strip() for l in table_text.strip().split('\n') if l.strip() and l.strip().startswith('|')]
        
        if len(lines) < 3:  # Need at least header + separator + 1 data row
            return None
        
        # Parse headers
        headers = [cell.strip() for cell in lines[0].split('|')[1:-1]]
        
        # Parse alignments from separator line
        sep_line = lines[1]
        alignments = []
        for cell in sep_line.split('|')[1:-1]:
            cell = cell.strip()
            if cell.startswith(':') and cell.endswith(':'):
                alignments.append('center')
            elif cell.endswith(':'):
                alignments.append('right')
            else:
                alignments.append('left')
        
        # Parse data rows
        rows = []
        for line in lines[2:]:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]
            rows.append(cells)
        
        return {
            'headers': headers,
            'alignments': alignments,
            'rows': rows
        }
    
    def _render_word_table(self, doc: Document, table_data: Dict[str, Any]):
        """
        Render markdown table as native Word table
        Enhanced with: zebra striping, vertical alignment, header styling
        """
        from docx.shared import Pt, RGBColor
        from docx.oxml import OxmlElement
        from docx.oxml.ns import qn
        from docx.enum.table import WD_ALIGN_VERTICAL
        
        # Create table
        table = doc.add_table(rows=1 + len(table_data['rows']), 
                              cols=len(table_data['headers']))
        table.style = 'Light Grid Accent 1'
        
        # Set column widths intelligently
        from docx.shared import Inches
        for i, header in enumerate(table_data['headers']):
            # STT column: narrow (0.5 inch)
            if header.strip().upper() in ['STT', 'TT']:
                table.columns[i].width = Inches(0.5)
            # Other columns: auto-adjust based on header length
            elif len(header) < 15:
                table.columns[i].width = Inches(1.5)
            else:
                table.columns[i].width = Inches(2.5)
        
        # Add borders and shading helper
        def set_cell_border(cell, **kwargs):
            """Set cell borders"""
            tc = cell._element
            tcPr = tc.get_or_add_tcPr()
            tcBorders = OxmlElement('w:tcBorders')
            for edge in ('top', 'left', 'bottom', 'right'):
                if edge in kwargs:
                    edge_element = OxmlElement(f'w:{edge}')
                    edge_element.set(qn('w:val'), 'single')
                    edge_element.set(qn('w:sz'), '4')
                    edge_element.set(qn('w:color'), '000000')
                    tcBorders.append(edge_element)
            tcPr.append(tcBorders)
        
        def set_cell_shading(cell, color):
            """Set cell background color"""
            shading_elm = OxmlElement('w:shd')
            shading_elm.set(qn('w:fill'), color)
            cell._element.get_or_add_tcPr().append(shading_elm)
        
        # Render headers with enhanced styling
        for i, (header, alignment) in enumerate(zip(table_data['headers'], table_data['alignments'])):
            cell = table.rows[0].cells[i]
            
            # Handle bold in header (**text**)
            if '**' in header:
                header = header.replace('**', '')
                run = cell.paragraphs[0].add_run(header)
                run.bold = True
            else:
                cell.text = header
                cell.paragraphs[0].runs[0].bold = True
            
            # Alignment
            if alignment == 'center':
                cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
            elif alignment == 'right':
                cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.RIGHT
            
            # Vertical center alignment
            cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
            
            # Header styling: dark blue background + white text
            set_cell_shading(cell, '4472C4')  # Dark blue
            for paragraph in cell.paragraphs:
                for run in paragraph.runs:
                    run.font.color.rgb = RGBColor(255, 255, 255)
                    run.font.size = Pt(11)
                    run.bold = True
            
            # Borders
            set_cell_border(cell, top={}, left={}, bottom={}, right={})
        
        # Render data rows with vertical merge support
        vertical_merges = {}  # Track {(col_idx, start_row): span_count}
        
        for row_idx, row_data in enumerate(table_data['rows'], start=1):
            for col_idx, cell_text in enumerate(row_data):
                cell = table.rows[row_idx].cells[col_idx]
                
                # Check for vertical merge markers
                vspan_match = None
                is_vspan_cont = '[VSPAN_CONT]' in cell_text
                
                if '[VSPAN=' in cell_text:
                    import re
                    vspan_match = re.search(r'\[VSPAN=(\d+)\]', cell_text)
                    if vspan_match:
                        span_count = int(vspan_match.group(1))
                        vertical_merges[(col_idx, row_idx)] = span_count
                        # Remove marker from text
                        cell_text = re.sub(r'\[VSPAN=\d+\]', '', cell_text).strip()
                
                # Skip continuation cells (will be merged)
                if is_vspan_cont:
                    continue
                
                # Check if this is a merged header row (bold + empty cells after)
                is_header_row = '**' in cell_text and any(c == '' for c in row_data[col_idx+1:] if col_idx < len(row_data)-1)
                
                # Handle multiline (<br/>) - Convert to proper line breaks
                if '<br/>' in cell_text:
                    # Convert <br/> tags to proper text formatting
                    cell_text = self._convert_html_tags_to_text(cell_text)
                    lines_in_cell = cell_text.split('\n')
                    
                    # Clear existing content and add first line
                    if lines_in_cell:
                        cell.text = lines_in_cell[0]
                        # Add additional lines as new paragraphs
                        for line in lines_in_cell[1:]:
                            if line.strip():  # Skip empty lines
                                cell.add_paragraph(line.strip())
                    
                    # Handle bold formatting
                    for paragraph in cell.paragraphs:
                        paragraph_text = paragraph.text
                        if '**' in paragraph_text:
                            # Clear paragraph and re-add with formatting
                            paragraph.clear()
                            bold_parts = paragraph_text.split('**')
                            for i, part in enumerate(bold_parts):
                                if part:
                                    run = paragraph.add_run(part)
                                    run.bold = (i % 2 == 1)  # Odd indices are bold
                # Handle bold (for cells without <br/> tags)
                elif '**' in cell_text:
                    # Clear cell and re-add with formatting
                    cell.text = ''  # Clear existing text
                    bold_parts = cell_text.split('**')
                    for i, part in enumerate(bold_parts):
                        if part:
                            run = cell.paragraphs[0].add_run(part)
                            run.bold = (i % 2 == 1)  # Odd indices are bold
                else:
                    # Plain text - also convert any HTML tags
                    cell.text = self._convert_html_tags_to_text(cell_text)
                
                # Alignment
                alignment = table_data['alignments'][col_idx]
                if alignment == 'center':
                    cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
                elif alignment == 'right':
                    cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.RIGHT
                
                # Vertical center alignment
                cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
                
                # Zebra striping: light blue for even rows
                if row_idx % 2 == 0:
                    set_cell_shading(cell, 'D9E9F7')  # Light blue
                
                # Borders
                set_cell_border(cell, top={}, left={}, bottom={}, right={})
                
                # Merge empty cells after header
                if is_header_row and col_idx < len(row_data) - 1:
                    # Find consecutive empty cells
                    merge_to = col_idx
                    for j in range(col_idx + 1, len(row_data)):
                        if row_data[j] == '':
                            merge_to = j
                        else:
                            break
                    
                    if merge_to > col_idx:
                        # Merge cells
                        cell.merge(table.rows[row_idx].cells[merge_to])
        
        # Execute vertical merges
        for (col_idx, start_row), span_count in vertical_merges.items():
            try:
                # Merge from start_row to start_row + span_count - 1
                start_cell = table.rows[start_row].cells[col_idx]
                end_cell = table.rows[start_row + span_count - 1].cells[col_idx]
                start_cell.merge(end_cell)
            except Exception as e:
                logger.warning(f"Failed to merge vertical cells at col {col_idx}, row {start_row}: {e}")
    
    def create_word_document(
        self,
        text: str,
        output_path: Path,
        title: str = "Káº¿t quáº£ OCR"
    ) -> Path:
        """
        Táº¡o file Word tá»« text vá»›i format Ä‘áº¹p
        """
        try:
            doc = Document()
            
            # Add title
            heading = doc.add_heading(title, level=1)
            heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # Add metadata
            info_para = doc.add_paragraph()
            info_para.add_run(f"NgÃ y táº¡o: {datetime.now().strftime('%d/%m/%Y %H:%M')}\n").italic = True
            info_para.add_run(f"Nguá»“n: TrÃ­ch xuáº¥t tá»± Ä‘á»™ng báº±ng AI\n").italic = True
            info_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # Add separator
            doc.add_paragraph("â”€" * 60)
            
            # Parse and render content with table support
            import re
            
            # Split text by table markers
            parts = re.split(r'(\[TABLE\].*?\[/TABLE\])', text, flags=re.DOTALL)
            
            for part in parts:
                part = part.strip()
                if not part:
                    continue
                
                # Check if this is a table
                if part.startswith('[TABLE]') and part.endswith('[/TABLE]'):
                    # Extract table content
                    table_content = part[7:-8].strip()  # Remove [TABLE] and [/TABLE]
                    
                    try:
                        # Parse markdown table
                        table_data = self._parse_markdown_table(table_content)
                        
                        if table_data:
                            # Render as native Word table
                            self._render_word_table(doc, table_data)
                            doc.add_paragraph()  # Add spacing after table
                        else:
                            # Fallback: render as plain text
                            doc.add_paragraph(table_content)
                    except Exception as e:
                        logger.warning(f"Failed to parse table, rendering as text: {e}")
                        doc.add_paragraph(table_content)
                else:
                    # Regular text - convert HTML-like tags to proper formatting
                    # Fix: Convert <br/> tags to actual line breaks
                    processed_part = self._convert_html_tags_to_text(part)
                    
                    # Regular text - preserve paragraphs
                    for paragraph_text in processed_part.split('\n'):
                        if paragraph_text.strip():
                            p = doc.add_paragraph(paragraph_text)
                            # Set font (Arial 12pt for Vietnamese)
                            for run in p.runs:
                                run.font.name = 'Arial'
                                run.font.size = Pt(12)
            
            # Save
            doc.save(str(output_path))
            logger.info(f"âœ… Word document created: {output_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"Failed to create Word document: {e}")
            raise

    def _convert_html_tags_to_text(self, text: str) -> str:
        """
        Convert HTML-like tags and entities to plain text formatting
        
        IMPROVED LOGIC:
        1. Detect issues BEFORE processing (better accuracy)
        2. Smart tag handling (preserve content, remove only structural tags)
        3. Careful entity conversion (avoid content loss)  
        4. Unicode normalization for Vietnamese
        5. Comprehensive logging for improvement
        
        Converts:
        - <br/>, <br> â†’ \n (line breaks)
        - HTML entities â†’ Unicode (safe conversion)
        - Structural tags â†’ removed (content preserved)
        - Vietnamese characters â†’ properly normalized
        """
        import re
        import html
        import unicodedata
        
        # Store original for comparison and detection
        original_text = text
        
        # Step 1: Detect issues BEFORE processing (more accurate)
        self._detect_unhandled_html_patterns(text, "BEFORE_PROCESSING")
        
        # Step 2: Convert <br/> and <br> tags to newlines first (safe operation)
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # Step 3: Smart HTML entity conversion
        try:
            # Use html.unescape for comprehensive conversion (600+ entities)
            unescaped_text = html.unescape(text)
            if unescaped_text != text:
                changed_chars = abs(len(text) - len(unescaped_text))
                logger.info(f"ğŸ”„ HTML entities converted: {changed_chars} character changes")
            text = unescaped_text
            
        except Exception as e:
            logger.warning(f"HTML unescape failed: {e}, using manual fallback")
            # Fallback: Manual conversion for critical entities
            critical_entities = {
                '&nbsp;': ' ', '&amp;': '&', '&lt;': '<', '&gt;': '>',
                '&quot;': '"', '&#39;': "'", '&apos;': "'",
                # Vietnamese critical entities
                '&aacute;': 'Ã¡', '&agrave;': 'Ã ', '&acirc;': 'Ã¢', '&atilde;': 'Ã£',
                '&eacute;': 'Ã©', '&egrave;': 'Ã¨', '&ecirc;': 'Ãª', 
                '&iacute;': 'Ã­', '&igrave;': 'Ã¬', '&icirc;': 'Ã®',
                '&oacute;': 'Ã³', '&ograve;': 'Ã²', '&ocirc;': 'Ã´', '&otilde;': 'Ãµ',
                '&uacute;': 'Ãº', '&ugrave;': 'Ã¹', '&ucirc;': 'Ã»',
                '&yacute;': 'Ã½', '&ygrave;': 'á»³', '&ycirc;': 'Å·',
            }
            for entity, replacement in critical_entities.items():
                text = text.replace(entity, replacement)
        
        # Step 4: Smart HTML tag removal (preserve content, remove only structural tags)
        # Remove common structural tags but keep content
        structural_tags = ['p', 'div', 'span', 'strong', 'em', 'b', 'i', 'u', 'section', 'article']
        for tag in structural_tags:
            # Remove opening and closing tags, keep content
            text = re.sub(f'</?{tag}[^>]*>', '', text, flags=re.IGNORECASE)
        
        # Remove any remaining well-formed HTML tags (but be careful with < > in content)  
        # Only remove if it looks like a proper HTML tag (starts with letter)
        text = re.sub(r'</?[a-zA-Z][^>]*>', '', text)
        
        # Step 5: Vietnamese Unicode normalization (important for consistency)
        try:
            text = unicodedata.normalize('NFC', text)
        except Exception as e:
            logger.warning(f"Unicode normalization failed: {e}")
        
        # Step 6: Clean up whitespace intelligently
        # Multiple newlines â†’ double newline (preserve paragraph structure)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  
        # Multiple spaces â†’ single space
        text = re.sub(r'[ \t]+', ' ', text)
        # Clean up space around newlines
        text = re.sub(r' *\n *', '\n', text)
        # Trim start/end whitespace
        text = text.strip()
        
        # Step 7: Final detection of remaining issues (for improvement)
        self._detect_unhandled_html_patterns(text, "AFTER_PROCESSING")
        
        return text
    
    def _detect_unhandled_html_patterns(self, processed_text: str, stage: str):
        """
        Detect HTML entities/tags that weren't handled properly
        Enhanced with stage detection for better debugging
        """
        import re
        
        # Detect remaining HTML entities (&#xxx; or &name;)
        remaining_entities = re.findall(r'&[a-zA-Z0-9#]+;', processed_text)
        if remaining_entities:
            unique_entities = list(set(remaining_entities))[:5]  # Show max 5
            logger.warning(f"âš ï¸  [{stage}] HTML entities: {unique_entities}")
        
        # Detect remaining HTML tags (but exclude false positives like mathematical < >)
        # Only flag if it looks like proper HTML tags
        remaining_tags = re.findall(r'<[a-zA-Z/][^>]*>', processed_text)
        if remaining_tags:
            unique_tags = list(set(remaining_tags))[:3]  # Show max 3
            logger.warning(f"âš ï¸  [{stage}] HTML tags: {unique_tags}")
        
        # Detect potential encoding issues specific to Vietnamese
        vietnamese_encoding_issues = re.findall(r'[Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µ][?ï¿½]|[?ï¿½][Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µ]', processed_text)
        if vietnamese_encoding_issues:
            logger.warning(f"âš ï¸  [{stage}] Vietnamese encoding issues: {vietnamese_encoding_issues[:3]}")
        
        # Summary logging
        total_issues = len(remaining_entities) + len(remaining_tags) + len(vietnamese_encoding_issues)
        if total_issues > 0 and stage == "AFTER_PROCESSING":
            logger.info(f"ğŸ“Š Processing complete: {total_issues} patterns need attention")
